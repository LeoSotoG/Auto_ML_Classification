{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWQkKZAvReMc8vXZQwmAYy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoSotoG/Auto_ML_Classification/blob/main/Auto_ML_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Proyect**\n",
        "\n",
        "El conjunto de datos contiene información sobre clientes bancarios que abandonaron el banco o continúan siendo clientes. Y se planteara un modelo  de rotación de clientes. El modelo de predicción de rotación de clientes bancarios se utiliza para prever qué clientes tienen más probabilidades de abandonar el banco en el futuro.  "
      ],
      "metadata": {
        "id": "gbJIWnlHMtcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Column Information\n",
        "El conjunto de datos incluye los siguientes atributos:\n",
        "\n",
        "* **Customer ID:** Un identificador único para cada cliente.\n",
        "* **Surname:** El apellido o apellido del cliente.\n",
        "* **Credit Score:** Un valor numérico que representa la puntuación crediticia del cliente.\n",
        "* **Geography:** El país donde reside el cliente (Francia, España o Alemania).\n",
        "* **Gender:** El género del cliente (Masculino o Femenino).\n",
        "* **Age:** La edad del cliente.\n",
        "* **Tenure:** El número de años que el cliente ha estado con el banco.\n",
        "* **Balance:** El saldo de la cuenta del cliente.\n",
        "* **NumOfProducts:** El número de productos bancarios que utiliza el cliente (por ejemplo, cuenta de ahorro, tarjeta de crédito).\n",
        "* **HasCrCard:** Si el cliente tiene una tarjeta de crédito (1 = sí, 0 = no).\n",
        "* **IsActiveMember:** Si el cliente es un miembro activo (1 = sí, 0 = no).\n",
        "* **EstimatedSalary:** El salario estimado del cliente.\n",
        "* **Exited:** Si el cliente ha abandonado (1 = sí, 0 = no)."
      ],
      "metadata": {
        "id": "31gX51XWM4Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load libraries"
      ],
      "metadata": {
        "id": "EhAfpibuM3RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Para imputación\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Para el preprocesamiento\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer\n",
        "\n",
        "# Para modelado\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Medir tiempo de ejecución\n",
        "import time\n",
        "\n",
        "# Configurar pandas para mostrar números en formato estándar\n",
        "pd.options.display.float_format = \"{:.2f}\".format"
      ],
      "metadata": {
        "id": "DDRnWnTbMxu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data"
      ],
      "metadata": {
        "id": "hEx9NR26LJkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"Churn_Modelling.csv\")"
      ],
      "metadata": {
        "id": "WbS7VPEkLLZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data processing"
      ],
      "metadata": {
        "id": "c5cUrf3cLTxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Customer ID:** Si bien es un identificador único para cada cliente, no proporciona información significativa sobre el comportamiento del cliente ni sobre los factores que podrían influir en su decisión de abandonar el banco. Por lo tanto, incluirlo en el modelo probablemente no mejorarí­a su capacidad predictiva y podría incluso introducir ruido en los datos.\n",
        "* **Surname:** El apellido del cliente tampoco suele ser relevante para predecir la rotación de clientes. Al igual que el ID del cliente, el apellido no proporciona información directa sobre las características o el comportamiento del cliente que podrían estar relacionadas con la rotación. Por lo tanto, tampoco suele incluirse en el modelo."
      ],
      "metadata": {
        "id": "KJdYU9yWLXkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split variables"
      ],
      "metadata": {
        "id": "78eTqqDALa37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = [\"Geography\", \"Gender\"]"
      ],
      "metadata": {
        "id": "0lqDPkZ7LUot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\"]"
      ],
      "metadata": {
        "id": "6L_zj9w3LiZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imputation of null data"
      ],
      "metadata": {
        "id": "ORJA53TnLxbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variables nulas categoricas (Prueba CHI Cuadrada)**"
      ],
      "metadata": {
        "id": "n7XRmqICMAv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables a imputar categoricas\n",
        "vc_nulas = [\"Geography\"]"
      ],
      "metadata": {
        "id": "SXgfEVbuLz67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir imputador por moda\n",
        "imputer = SimpleImputer(strategy = \"most_frequent\")"
      ],
      "metadata": {
        "id": "waFuoBrvMGju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Prueba CHI Cuadrada)"
      ],
      "metadata": {
        "id": "CPDq2yk7MwZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in vc_nulas:\n",
        "  X = data[[column]]\n",
        "  Xi = pd.DataFrame(imputer.fit_transform(X), columns = [column])\n",
        "  # Creamos una tabla de contingencia\n",
        "  tabla_contingencia = pd.crosstab(X[column].dropna(), Xi[column])\n",
        "  # Aplicamos la prueba de Chi-cuadrado\n",
        "  chi2, p_valor, _, _ = chi2_contingency(tabla_contingencia)\n",
        "  #Decision\n",
        "  if p_valor < 0.05:\n",
        "    print(f\"La imputación en la columna {column} fue correcta\")\n",
        "    # Haciendo la imputacion en el DataFrame original\n",
        "    imputed_values = imputer.transform(data[[column]].copy())\n",
        "    imputed_df = pd.DataFrame(imputed_values, columns=[column], index=data.index)\n",
        "    data[column] = imputed_df\n",
        "  else:\n",
        "    print(f\"La imputación en la columna {column} NO fue correcta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM0orEPqMJOh",
        "outputId": "a446f30a-5b5b-49fe-d444-b993ea2c18a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La imputación en la columna Geography fue correcta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variables nulas numericas (Prueba de Kolmogorov-Smirnov)**"
      ],
      "metadata": {
        "id": "uC94mXZhMNSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable a imputar numericas\n",
        "vn_nulas = [\"Age\", \"HasCrCard\",\"IsActiveMember\"]"
      ],
      "metadata": {
        "id": "JqqPwfk2MOsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir imputador por promedio\n",
        "im = SimpleImputer(strategy = \"mean\")"
      ],
      "metadata": {
        "id": "PFKqXkR_Mp27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Prueba de Kolmogorov-Smirnov)"
      ],
      "metadata": {
        "id": "DjkFUgQLMtCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in vn_nulas:\n",
        "  # Descargando la columna\n",
        "  X = data[[col]]\n",
        "  # Imputando los datos\n",
        "  Xi = pd.DataFrame(im.fit_transform(X), columns=[col])\n",
        "  #Estadistico de prueba\n",
        "  if ks_2samp(X[col].dropna(),Xi[col]).statistic < 0.1:\n",
        "    print(f\"La imputación en la columna {col} fue correcta\")\n",
        "    # Haciendo la imputacion en el DataFrame original\n",
        "    data[col] = im.transform(data[[col]].copy())\n",
        "  else:\n",
        "    print(f\"La imputación en la columna {col} NO fue correcta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqiwh269Msd9",
        "outputId": "ec110820-02f4-4a3b-f76e-7cc1784c6864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La imputación en la columna Age fue correcta\n",
            "La imputación en la columna HasCrCard fue correcta\n",
            "La imputación en la columna IsActiveMember fue correcta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Age\"].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX0E3J1BM6Bs",
        "outputId": "b32ee8a8-8b2f-42d7-cb76-906c350db2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Outliers"
      ],
      "metadata": {
        "id": "5qBzZQWSM8PL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###IQR"
      ],
      "metadata": {
        "id": "cgnaJwmWNAKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables a imputar\n",
        "var_outliers = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\"]\n",
        "\n",
        "numerical_data = data[numerical_features]"
      ],
      "metadata": {
        "id": "sMmFcZHCM9Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantile 1 y 3\n",
        "Q1 = numerical_data.quantile(0.25)\n",
        "Q3 = numerical_data.quantile(0.75)\n",
        "\n",
        "# Obteniendo el IQR\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Inicializar una máscara para todas las filas\n",
        "mask_total = pd.Series(True, index=data.index)\n",
        "\n",
        "# Calcular y combinar las máscaras para cada columna\n",
        "for col in var_outliers:\n",
        "    mask = ~((numerical_data[col] < (Q1[col] - 1.5 * IQR[col])) | (numerical_data[col] > (Q3[col] + 1.5 * IQR[col])))\n",
        "    # Alinear las máscaras para asegurarse de que tengan el mismo índice\n",
        "    mask = mask.reindex(mask_total.index, fill_value=False)\n",
        "    # Combinar las máscaras\n",
        "    mask_total &= mask\n",
        "\n",
        "len(numerical_data), len(numerical_data[mask_total])\n",
        "# Se perdieron 432 datos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4_SmBBfNK12",
        "outputId": "c9a8d291-1c12-4679-f468-dd8d0e3b71dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10002, 9570)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando estos cambios\n",
        "data = data[mask_total]\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmMrHd7zNM_1",
        "outputId": "bf62eb5a-5682-4752-a72e-5036b3e9d8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9570, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipeline"
      ],
      "metadata": {
        "id": "zhPCzyObNTur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funcion para buscar los hiperparametros de los modelos y encontrar los optimos, utiliza programación paralela.**"
      ],
      "metadata": {
        "id": "s2F8Ic34NwF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entrenar(param, modelo, X, y):\n",
        "    # Inicia una búsqueda aleatoria de hiperparámetros\n",
        "    grid = RandomizedSearchCV(param_distributions=param,\n",
        "                              # Utiliza todos los núcleos disponibles para procesamiento paralelo\n",
        "                              n_jobs=-1,\n",
        "                              # Número de iteraciones de búsqueda aleatoria\n",
        "                              n_iter=10,\n",
        "                              # Número de divisiones para la validación cruzada\n",
        "                              cv=4,\n",
        "                              # Estimador del modelo a utilizar\n",
        "                              estimator=modelo,\n",
        "                              # Cómo manejar errores durante el ajuste del modelo\n",
        "                              error_score='raise')\n",
        "\n",
        "    # Ajusta el modelo utilizando búsqueda aleatoria de hiperparámetros\n",
        "    grid.fit(X, y)\n",
        "\n",
        "    # Retorna los resultados de la búsqueda, el mejor estimador, el mejor puntaje, y los mejores parámetros\n",
        "    return grid, grid.best_estimator_, grid.best_score_, grid.best_params_"
      ],
      "metadata": {
        "id": "z7XhgbFrNVAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funcion para evaluar el modelo**"
      ],
      "metadata": {
        "id": "J1QbAJPtObYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metricas(Xt, Xv, yt, yv, modelo):\n",
        "    # Define una función llamada metricas que toma como entrada los conjuntos de datos de entrenamiento (Xt, yt),\n",
        "    # los conjuntos de datos de validación (Xv, yv) y el modelo entrenado (modelo).\n",
        "\n",
        "    d = {'train': round(roc_auc_score(y_true=yt, y_score=modelo.predict_proba(Xt)[:,1]), 3),\n",
        "         # Calcula el área bajo la curva ROC (AUC) para el conjunto de entrenamiento\n",
        "         # y lo almacena en un diccionario con la clave 'train'.\n",
        "         # Utiliza predict_proba para obtener las probabilidades predichas y selecciona la columna correspondiente\n",
        "         # al valor positivo (columna 1) para calcular el AUC.\n",
        "\n",
        "         'validate': round(roc_auc_score(y_true=yv, y_score=modelo.predict_proba(Xv)[:,1]), 3)\n",
        "         # Calcula el área bajo la curva ROC (AUC) para el conjunto de validación\n",
        "         # y lo almacena en un diccionario con la clave 'validate'.\n",
        "         # Utiliza predict_proba para obtener las probabilidades predichas y selecciona la columna correspondiente\n",
        "         # al valor positivo (columna 1) para calcular el AUC.\n",
        "        }\n",
        "    return d\n",
        "    # Retorna el diccionario que contiene los valores de AUC para los conjuntos de entrenamiento y validación."
      ],
      "metadata": {
        "id": "9uSqYxunOctN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargando las variables numericas\n",
        "numerical_data = data[numerical_features]\n",
        "\n",
        "# Descargando las variables categoricas\n",
        "categorical_data = data[categorical_features]"
      ],
      "metadata": {
        "id": "BeG6gfZ4Q7WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definiendo la target\n",
        "y = data[\"Exited\"]"
      ],
      "metadata": {
        "id": "MO6FZLEjUKXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hiperarametros"
      ],
      "metadata": {
        "id": "c-O9jIfFYbOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperarametros del modelo de Regresión Logistica\n",
        "param_logreg = dict(penalty = ['l1', 'l2'],  # Define la norma utilizada en la regularización: l1 (valor absoluto) y l2 (cuadrado)\n",
        "                     C = np.arange(0.1, 2, 0.1),  # Define una lista de valores para el parámetro de regularización C, que va desde 0.1 hasta 2 en incrementos de 0.1\n",
        "                     solver = ['liblinear', 'saga'],  # Define los solvers utilizados para optimizar la función de coste: 'liblinear' para problemas pequeños y 'saga' para problemas grandes\n",
        "                     max_iter = [100, 200, 300],  # Define el número máximo de iteraciones\n",
        "                     random_state = [42]  # Define una semilla aleatoria para reproducibilidad\n",
        "                    )"
      ],
      "metadata": {
        "id": "V863RtRyiZDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperarametros del modelo Arbol de Decision\n",
        "param_arbol_decision = dict(\n",
        "    criterion = ['gini', 'entropy'],  # Criterio para medir la calidad de una división\n",
        "    max_depth = [None] + list(range(2, 5)),  # Profundidad máxima del árbol\n",
        "    min_samples_split = list(range(2, 4)),  # Número mínimo de muestras requeridas para dividir un nodo interno\n",
        "    min_samples_leaf = list(range(2, 4)),  # Número mínimo de muestras requeridas para ser una hoja\n",
        "    max_features = [None] + [i * .05 for i in list(range(2, 4))],  # Número máximo de características a considerar para la división\n",
        "    max_leaf_nodes = [None] + list(range(2, 10)),  # Número máximo de nodos hoja\n",
        "    min_impurity_decrease = [x * .10 for x in list(range(2, 4))]  # Un nodo se dividirá si esta división induce una disminución de la impureza mayor o igual a este valor\n",
        ")\n"
      ],
      "metadata": {
        "id": "MCaX6i2bk9rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperarametros del modelo KNN\n",
        "paramknn = dict(n_neighbors= (range(2, 4)),  # Número de vecinos a considerar\n",
        "          weights=[\"uniform\", \"distance\"],  # Método de ponderación de los vecinos\n",
        "          metric= [\"euclidean\", \"manhattan\"],  # Métrica de distancia utilizada\n",
        "          algorithm= [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],  # Algoritmo utilizado para calcular los vecinos más cercanos\n",
        "          p= [1, 2]  # Parámetro de potencia para la distancia de Minkowski\n",
        "          )"
      ],
      "metadata": {
        "id": "kbDwDdUTVRdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperarametros del modelo Random Forest\n",
        "param_RF = dict(n_estimators=list(range(1, 100, 25)),\n",
        "                                    criterion=['gini', 'entropy'],\n",
        "                                    max_depth=[x for x in list(range(2, 5))] + [None],\n",
        "                                    min_samples_split=[x for x in list(range(2, 4))],\n",
        "                                    min_samples_leaf=[x for x in list(range(2, 4))],\n",
        "                                    max_features=[None] + [i * .05 for i in list(range(2, 4))],\n",
        "                                    max_leaf_nodes=list(range(2, 10)) + [None],\n",
        "                                    min_impurity_decrease=[x * .10 for x in list(range(2, 4))],\n",
        "                                    oob_score=[True,False],\n",
        "                                    warm_start=[True, False],\n",
        "                                    class_weight=[None, 'balanced'],\n",
        "                                    max_samples=[None],)"
      ],
      "metadata": {
        "id": "wlSl_ZUPjVyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparametros del modelo Ada Boost\n",
        "param_adab = dict(n_estimators = range(2,10),\n",
        "             learning_rate = np.arange(0.1,1,0.1),\n",
        "             algorithm = ['SAMME.R'])"
      ],
      "metadata": {
        "id": "1nvwVsBKizIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparametros del modelo GradientBoostingClassifier\n",
        "param_gradient_boosting = dict(\n",
        "    n_estimators = list(range(1, 100, 25)),  # Número de árboles de decisión a construir\n",
        "    learning_rate = [0.001, 0.01, 0.1],  # Tasa de aprendizaje\n",
        "    max_depth = [3, 4, 5],  # Profundidad máxima de los árboles de decisión\n",
        "    min_samples_split = list(range(2, 4)),  # Número mínimo de muestras requeridas para dividir un nodo interno\n",
        "    min_samples_leaf = list(range(1, 3)),  # Número mínimo de muestras requeridas para ser una hoja\n",
        "    subsample = [0.5, 0.8, 1.0],  # Fracción de muestras utilizadas para ajustar los árboles de decisión\n",
        "    max_features = [None, 'sqrt', 'log2'],  # Número máximo de características a considerar para la división\n",
        "    max_leaf_nodes = [None, 5, 10],  # Número máximo de nodos hoja\n",
        "    min_impurity_decrease = [0.0, 0.1, 0.2]  # Un nodo se dividirá si esta división induce una disminución de la impureza mayor o igual a este valor\n",
        ")\n"
      ],
      "metadata": {
        "id": "3Krz5obOkVoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Escaladores"
      ],
      "metadata": {
        "id": "RSWEKt-1Yd6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de nombres de los escaladores\n",
        "nombres_escaladores = [\"standard\",\n",
        "                       \"minmax\",\n",
        "                       \"robust\",\n",
        "                       \"maxabs\"]"
      ],
      "metadata": {
        "id": "wKyx5PxLOdOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Modelos"
      ],
      "metadata": {
        "id": "TELt4S8kdlMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de nombres de los modelos\n",
        "nombres_modelos = [\"Regresión Logística\",\n",
        "                   \"Árbol de Decisión\",\n",
        "                   \"KNN\",\n",
        "                   \"Random Forest\",\n",
        "                   \"Ada Boost\",\n",
        "                   \"Gradient Boosting Machines\"]"
      ],
      "metadata": {
        "id": "_cDAhPbZdkz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline**"
      ],
      "metadata": {
        "id": "guH7fTObTnwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicia el tiempo total de ejecución\n",
        "#total_start_time = time.time()\n",
        "\n",
        "for model in nombres_modelos:\n",
        "  # Aplicar modelo correspondiente\n",
        "  if model == \"Regresión Logística\":\n",
        "    modelo = LogisticRegression()\n",
        "    param = param_logreg\n",
        "  elif model == \"Árbol de Decisión\":\n",
        "    modelo = DecisionTreeClassifier()\n",
        "    param = param_arbol_decision\n",
        "  elif model == \"KNN\":\n",
        "    modelo = KNeighborsClassifier()\n",
        "    param = paramknn\n",
        "  elif model == \"Random Forest\":\n",
        "    modelo = RandomForestClassifier()\n",
        "    param = param_RF\n",
        "  elif model == \"Ada Boost\":\n",
        "    modelo = AdaBoostClassifier()\n",
        "    param = param_adab\n",
        "  elif model == \"Gradient Boosting Machines\":\n",
        "    modelo = GradientBoostingClassifier()\n",
        "    param = param_gradient_boosting\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"\")\n",
        "  print(\"       \",model)\n",
        "\n",
        "  for scaler_type in nombres_escaladores:\n",
        "      # Inicia el tiempo de ejecución del modelo actual\n",
        "      model_start_time = time.time()\n",
        "      # Aplicar escalado solo a las características numéricas\n",
        "      if scaler_type == \"standard\":\n",
        "          scaler = StandardScaler()\n",
        "      elif scaler_type == \"minmax\":\n",
        "          scaler = MinMaxScaler()\n",
        "      elif scaler_type == \"robust\":\n",
        "          scaler = RobustScaler()\n",
        "      elif scaler_type == \"maxabs\":\n",
        "          scaler = MaxAbsScaler()\n",
        "      elif scaler_type == \"quantile\":\n",
        "          scaler = QuantileTransformer(output_distribution=\"normal\")\n",
        "      else:\n",
        "          print(\"Tipo de escalado no válido.\")\n",
        "\n",
        "      # Escalando datos numericos\n",
        "      numerical_data_preprocessed = scaler.fit_transform(numerical_data)\n",
        "\n",
        "      # Creamos un codificador One-Hot\n",
        "      encoder = OneHotEncoder()\n",
        "\n",
        "      # Ajustamos y transformamos los datos categóricos utilizando One-Hot Encoding\n",
        "      encoded_data = encoder.fit_transform(categorical_data)\n",
        "\n",
        "      # Convertir la matriz sparse resultante en un DataFrame de pandas y nombrar las columnas\n",
        "      encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(categorical_features))\n",
        "\n",
        "      # Escalando datos categoricos\n",
        "      categorical_data_preprocessed = scaler.fit_transform(encoded_df)\n",
        "\n",
        "      # Concatenando los datos numericos y categoricos escalados\n",
        "      mixed_data = pd.concat([pd.DataFrame(numerical_data_preprocessed), pd.DataFrame(categorical_data_preprocessed)], axis=1)\n",
        "\n",
        "      # Asiganando de nuevo los nombres de las columnas\n",
        "      mixed_data.columns = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\",\n",
        "        \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\",\n",
        "        \"Geography_France\", \"Geography_Germany\", \"Geography_Spain\",\n",
        "        \"Gender_Female\", \"Gender_Male\"]\n",
        "\n",
        "      # Obteniendo datos de Entrenamiento y prueba\n",
        "      train, test, train_y, test_y =  train_test_split(mixed_data, y, test_size=0.30)\n",
        "\n",
        "      # Finaliza el tiempo de ejecución del modelo actual\n",
        "      model_end_time = time.time()\n",
        "      model_execution_time = model_end_time - model_start_time\n",
        "\n",
        "      modelo_nuevo, best_estimator, score, params = entrenar(param, modelo, train, train_y)\n",
        "      print(scaler_type)\n",
        "      print(metricas(train,test,train_y,test_y,modelo_nuevo))\n",
        "      print(f\"Tiempo de ejecución del modelo {model} con el escalamiento {scaler_type}: {model_execution_time:.2f} segundos\")\n",
        "      d = metricas(train,test,train_y,test_y,modelo_nuevo)\n",
        "\n",
        "      if abs(d[\"train\"] - d[\"validate\"]) < 0.01:  # Si las diferencias son pequeñas\n",
        "          print(\"El ajuste del modelo parece apropiado.\")\n",
        "      elif d[\"train\"] > d[\"validate\"]:  # Si la puntuación de entrenamiento es mayor\n",
        "          print(\"El modelo está sobreajustado (overfitting).\")\n",
        "      else:  # Si la puntuación de prueba es mayor\n",
        "          print(\"El modelo está subajustado (underfitting).\")\n",
        "      print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfLqGmePPtjR",
        "outputId": "9c1daa0e-faa5-45a5-c93a-63a817ce57f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "        Regresión Logística\n",
            "standard\n",
            "{'train': 0.79, 'validate': 0.793}\n",
            "Tiempo de ejecución del modelo Regresión Logística con el escalamiento standard: 0.03 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "minmax\n",
            "{'train': 0.787, 'validate': 0.804}\n",
            "Tiempo de ejecución del modelo Regresión Logística con el escalamiento minmax: 0.04 segundos\n",
            "El modelo está subajustado (underfitting).\n",
            "\n",
            "robust\n",
            "{'train': 0.792, 'validate': 0.791}\n",
            "Tiempo de ejecución del modelo Regresión Logística con el escalamiento robust: 0.03 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "maxabs\n",
            "{'train': 0.791, 'validate': 0.794}\n",
            "Tiempo de ejecución del modelo Regresión Logística con el escalamiento maxabs: 0.03 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "\n",
            "\n",
            "        Árbol de Decisión\n",
            "standard\n",
            "{'train': 0.5, 'validate': 0.5}\n",
            "Tiempo de ejecución del modelo Árbol de Decisión con el escalamiento standard: 0.03 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "minmax\n",
            "{'train': 0.5, 'validate': 0.5}\n",
            "Tiempo de ejecución del modelo Árbol de Decisión con el escalamiento minmax: 0.02 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "robust\n",
            "{'train': 0.5, 'validate': 0.5}\n",
            "Tiempo de ejecución del modelo Árbol de Decisión con el escalamiento robust: 0.03 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "maxabs\n",
            "{'train': 0.5, 'validate': 0.5}\n",
            "Tiempo de ejecución del modelo Árbol de Decisión con el escalamiento maxabs: 0.02 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "\n",
            "\n",
            "        KNN\n",
            "standard\n",
            "{'train': 0.968, 'validate': 0.706}\n",
            "Tiempo de ejecución del modelo KNN con el escalamiento standard: 0.02 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "minmax\n",
            "{'train': 0.97, 'validate': 0.697}\n",
            "Tiempo de ejecución del modelo KNN con el escalamiento minmax: 0.03 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "robust\n",
            "{'train': 0.947, 'validate': 0.735}\n",
            "Tiempo de ejecución del modelo KNN con el escalamiento robust: 0.03 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "maxabs\n",
            "{'train': 0.967, 'validate': 0.696}\n",
            "Tiempo de ejecución del modelo KNN con el escalamiento maxabs: 0.03 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "\n",
            "\n",
            "        Random Forest\n",
            "standard\n",
            "{'train': 0.5, 'validate': 0.5}\n",
            "Tiempo de ejecución del modelo Random Forest con el escalamiento standard: 0.02 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "minmax\n",
            "{'train': 0.5, 'validate': 0.5}\n",
            "Tiempo de ejecución del modelo Random Forest con el escalamiento minmax: 0.02 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "robust\n",
            "{'train': 0.5, 'validate': 0.5}\n",
            "Tiempo de ejecución del modelo Random Forest con el escalamiento robust: 0.04 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "maxabs\n",
            "{'train': 0.5, 'validate': 0.5}\n",
            "Tiempo de ejecución del modelo Random Forest con el escalamiento maxabs: 0.02 segundos\n",
            "El ajuste del modelo parece apropiado.\n",
            "\n",
            "\n",
            "\n",
            "        Ada Boost\n",
            "standard\n",
            "{'train': 0.843, 'validate': 0.828}\n",
            "Tiempo de ejecución del modelo Ada Boost con el escalamiento standard: 0.03 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "minmax\n",
            "{'train': 0.838, 'validate': 0.821}\n",
            "Tiempo de ejecución del modelo Ada Boost con el escalamiento minmax: 0.02 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "robust\n",
            "{'train': 0.83, 'validate': 0.84}\n",
            "Tiempo de ejecución del modelo Ada Boost con el escalamiento robust: 0.03 segundos\n",
            "El modelo está subajustado (underfitting).\n",
            "\n",
            "maxabs\n",
            "{'train': 0.836, 'validate': 0.823}\n",
            "Tiempo de ejecución del modelo Ada Boost con el escalamiento maxabs: 0.02 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "\n",
            "\n",
            "        Gradient Boosting Machines\n",
            "standard\n",
            "{'train': 0.869, 'validate': 0.852}\n",
            "Tiempo de ejecución del modelo Gradient Boosting Machines con el escalamiento standard: 0.02 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "minmax\n",
            "{'train': 0.839, 'validate': 0.849}\n",
            "Tiempo de ejecución del modelo Gradient Boosting Machines con el escalamiento minmax: 0.02 segundos\n",
            "El modelo está subajustado (underfitting).\n",
            "\n",
            "robust\n",
            "{'train': 0.886, 'validate': 0.861}\n",
            "Tiempo de ejecución del modelo Gradient Boosting Machines con el escalamiento robust: 0.02 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n",
            "maxabs\n",
            "{'train': 0.876, 'validate': 0.858}\n",
            "Tiempo de ejecución del modelo Gradient Boosting Machines con el escalamiento maxabs: 0.02 segundos\n",
            "El modelo está sobreajustado (overfitting).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LO_XIlwTq-Xx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}